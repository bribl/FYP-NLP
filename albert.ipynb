{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_dataset(args)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################\n",
    "## \n",
    "##             Imports\n",
    "## \n",
    "#####################################\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "#from cantokenizer import CanTokenizer\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math\n",
    "gelu_new_K = math.sqrt(2 / math.pi)\n",
    "\n",
    "def gelu_fast(x):\n",
    "    return 0.5 * x * (1 + torch.tanh(gelu_new_K * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \n",
    "\n",
    "\n",
    "#####################################\n",
    "## \n",
    "##            Data Utils\n",
    "## \n",
    "#####################################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "class textDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_path, \n",
    "                 seq_length,\n",
    "                 batch_size, \n",
    "                 eval=False, \n",
    "                 eval_num_samples=100000, \n",
    "                 cooked=None):\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.data_path = data_path\n",
    "        \n",
    "        self.eval_num_samples = (eval_num_samples // batch_size)\n",
    "        self.eval = eval\n",
    "        \n",
    "        try:\n",
    "            if cooked == False:\n",
    "                raise\n",
    "\n",
    "            self.length = os.stat(data_path+\"data_original_cooked\").st_size//(seq_length*2) // batch_size\n",
    "            path = data_path+\"data_original_cooked\"\n",
    "            with open(path, 'rb') as f:\n",
    "                pass\n",
    "            cooked = True\n",
    "        except:\n",
    "            if cooked == True:\n",
    "                raise Exception ('Not yet cooked')\n",
    "            self.length = os.stat(data_path+\"data_original\").st_size//(seq_length*2) // batch_size\n",
    "            cooked = False\n",
    "\n",
    "\n",
    "        self.actual_length = self.length\n",
    "\n",
    "\n",
    "        if self.length < eval_num_samples:\n",
    "            self.eval_num_samples = eval_num_samples = ((self.length // 10) // batch_size)\n",
    "\n",
    "        if eval:\n",
    "            self.length = eval_num_samples\n",
    "        \n",
    "            \n",
    "        self.cooked = cooked\n",
    "\n",
    "        self.ids_bin_buffer = None\n",
    "        self.dataset = None\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.eval_num_samples if self.eval else self.length - self.eval_num_samples\n",
    "    \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.dataset is None:\n",
    "            from data_utils import textDataset as _textDataset\n",
    "            self.dataset = _textDataset(\n",
    "                data_path=self.data_path, \n",
    "                seq_length=self.seq_length, \n",
    "                batch_size=self.batch_size, \n",
    "                eval=self.eval, \n",
    "                eval_num_samples=self.eval_num_samples, \n",
    "                cooked=self.cooked\n",
    "                )\n",
    "\n",
    "        return self.dataset.__getbatch__(i*self.batch_size, self.batch_size)\n",
    "\n",
    "    def __getbatch__(self, i):\n",
    "        return self.dataset(i, self.batch_size)\n",
    "\n",
    "\n",
    "#####################################\n",
    "## \n",
    "##             Modelling\n",
    "## \n",
    "#####################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AlbertForMaskedLM\n",
    "from transformers.activations import get_activation\n",
    "\n",
    "from transformers import AlbertConfig\n",
    "\n",
    "tokenizer = None\n",
    "\n",
    "    \n",
    "def get_model(args):\n",
    "    if args.model_size == 'debug':\n",
    "        num_hidden_layers = 1\n",
    "        embedding_size = 8\n",
    "        hidden_size = 16\n",
    "        intermediate_size = 32\n",
    "        num_attention_heads = 2\n",
    "        args.gen_ratio = 2\n",
    "\n",
    "    elif args.model_size == 'tiny':\n",
    "        num_hidden_layers = 4\n",
    "        embedding_size = 128\n",
    "        hidden_size = 336\n",
    "        intermediate_size = 1344\n",
    "        num_attention_heads = 12\n",
    "    elif args.model_size == 'small':\n",
    "        num_hidden_layers = 12\n",
    "        embedding_size = 128\n",
    "        hidden_size = 256\n",
    "        intermediate_size = 1024\n",
    "        num_attention_heads = 4\n",
    "    elif args.model_size == 'base':\n",
    "        num_hidden_layers = 12\n",
    "        embedding_size = 768\n",
    "        hidden_size = 768\n",
    "        intermediate_size = 3072\n",
    "        num_attention_heads = 12\n",
    "\n",
    "    else:\n",
    "        raise Exception('Which model? small, base, large')\n",
    "    \n",
    "\n",
    "    config = AlbertConfig(\n",
    "        max_position_embeddings=args.seq_length,\n",
    "        vocab_size=args.vocab_size,\n",
    "\n",
    "        num_hidden_layers=num_hidden_layers,\n",
    "        embedding_size=embedding_size,\n",
    "\n",
    "        hidden_size = hidden_size // args.gen_ratio,\n",
    "        intermediate_size = intermediate_size // args.gen_ratio,\n",
    "        num_attention_heads=num_attention_heads // args.gen_ratio,\n",
    "    )\n",
    "\n",
    "    model = AlbertForMaskedLM(\n",
    "        config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_loss(model, sample, args, device, gpu=0, report=False):\n",
    "\n",
    "    generator_input, generator_labels, generator_mask, generator_original = sample\n",
    "\n",
    "    if gpu:\n",
    "        generator_input = generator_input.to(device)\n",
    "        generator_labels = generator_labels.to(device)\n",
    "        generator_mask = generator_mask.to(device)\n",
    "\n",
    "\n",
    "    loss, scores = model(generator_input, \n",
    "                         attention_mask=generator_mask, \n",
    "                         labels=generator_labels)\n",
    "\n",
    "    total_loss = loss\n",
    "\n",
    "    log = None\n",
    "    if report:\n",
    "        log = OrderedDict()\n",
    "        log['loss'] = total_loss\n",
    "\n",
    "\n",
    "    return total_loss, log\n",
    "\n",
    "\n",
    "def log_formatter(log):\n",
    "    pass\n",
    "def get_tokenizer(args):\n",
    "    return CanTokenizer(vocab_file = args.vocab_file)\n",
    "\n",
    "def set_parser(parser):\n",
    "    parser.add_argument('--model-size', default='small',\n",
    "                        help='model size, '\n",
    "                                'e.g., \"small\", \"base\", \"large\" (default: small)')\n",
    "    parser.add_argument(\"--gen-ratio\", default=4, type=int,\n",
    "                        help=\"discriminator to generator ratio\")\n",
    "    parser.add_argument(\"--disc-weight\", default=50, type=int,\n",
    "                        help=\"discriminator loss scalar\")\n",
    "\n",
    "    parser.add_argument('--cooked', action='store_true', help='whether data are cooked')\n",
    "\n",
    "def get_dataset(args):\n",
    "    return textDataset(\n",
    "        args.data, \n",
    "        args.seq_length, \n",
    "        args.batch_size,\n",
    "        eval_num_samples=0,\n",
    "        cooked=args.cooked\n",
    "    )\n",
    "\n",
    "\n",
    "def get_eval_dataset(args):\n",
    "    pass\n",
    "\n",
    "def evaluate(model, sample, args, device, record, gpu=0, report=False):\n",
    "    pass\n",
    "\n",
    "def post_evaluate(record, args):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "get_model\n",
    "get_loss\n",
    "log_formatter \n",
    "get_tokenizer\n",
    "evaluate\n",
    "post_evaluate\n",
    "set_parser\n",
    "get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
